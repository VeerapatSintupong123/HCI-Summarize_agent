{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb4c9c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json \n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1969c569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will scrape news for these dates: ['2025-09-22', '2025-09-21', '2025-09-20', '2025-09-19', '2025-09-18', '2025-09-17', '2025-09-16']\n"
     ]
    }
   ],
   "source": [
    "# Generate 7 days date range (excluding today)\n",
    "def get_date_range(days=7):\n",
    "    dates = []\n",
    "    for i in range(1, days + 1):  # Start from 1 to exclude today\n",
    "        date = (datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d')\n",
    "        dates.append(date)\n",
    "    return dates\n",
    "\n",
    "date_list = get_date_range(7)\n",
    "print(f\"Will scrape news for these dates: {date_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ea8e6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = '7eb571b97f8440118c46dc8c74279e0e'\n",
    "QUERY = [\"NVIDIA\", \"AMD\", \"Intel\"]\n",
    "# No longer need DATE_FROM and DATE_NOW as we'll use individual dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96a6ce1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_news_for_single_day(api_key, query, target_date, max_articles=20):\n",
    "    \"\"\"\n",
    "    Fetch news for a specific date with limited articles\n",
    "    \"\"\"\n",
    "    if not is_valid_date(target_date):\n",
    "        raise ValueError(\"Date must be in YYYY-MM-DD format\")\n",
    "\n",
    "    # Use the same date for both from and to for single day\n",
    "    url = f\"https://newsapi.org/v2/everything?q={query}&from={target_date}&to={target_date}&language=en&apiKey={api_key}&pageSize={max_articles}&page=1\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        total_articles = data.get('totalResults', 0)\n",
    "        articles = data.get('articles', [])\n",
    "        \n",
    "        # Limit to max_articles\n",
    "        limited_articles = articles[:max_articles]\n",
    "        \n",
    "        return {\n",
    "            \"articles\": limited_articles,\n",
    "            \"total_available\": total_articles,\n",
    "            \"fetched_count\": len(limited_articles),\n",
    "            \"date\": target_date,\n",
    "            \"query\": query\n",
    "        }\n",
    "    else:\n",
    "        return {\"error\": f\"Failed to fetch news: {response.text}\"}\n",
    "\n",
    "def is_valid_date(date_string):\n",
    "    \"\"\"\n",
    "    Checks if a string can be parsed into a date according to a specific format.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        for fmt in (\"%Y-%m-%d\", \"%Y-%m-%dT%H:%M:%S\"):\n",
    "            try:\n",
    "                datetime.strptime(date_string, fmt)\n",
    "                return True\n",
    "            except ValueError:\n",
    "                continue\n",
    "        return False\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7fb4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Starting day-by-day news scraping...\")\n",
    "    print(f\"Target dates: {date_list}\")\n",
    "    print(f\"Companies: {', '.join(QUERY)}\")\n",
    "    print(f\"Max articles per company per day: 20\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    if not os.path.exists(r'../data/raw/news_api'):\n",
    "        os.makedirs(r'../data/raw/news_api')\n",
    "    \n",
    "    # Get today's date for filename\n",
    "    scrape_date = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Dictionary to store articles for each company\n",
    "    company_articles = {company: [] for company in QUERY}\n",
    "    \n",
    "    for target_date in date_list:\n",
    "        print(f\"\\nğŸ“… Scraping news for {target_date}\")\n",
    "        \n",
    "        for company in QUERY:\n",
    "            print(f\"  ğŸ” Fetching {company} news...\")\n",
    "            \n",
    "            # Fetch news for this specific day and company\n",
    "            result = fetch_news_for_single_day(API_KEY, company, target_date, max_articles=20)\n",
    "            \n",
    "            if \"error\" in result:\n",
    "                print(f\"  âŒ Error fetching {company} news: {result['error']}\")\n",
    "                continue\n",
    "            \n",
    "            articles = result[\"articles\"]\n",
    "            total_available = result[\"total_available\"]\n",
    "            fetched_count = result[\"fetched_count\"]\n",
    "            \n",
    "            print(f\"  ğŸ“Š {company}: {fetched_count} articles fetched (out of {total_available} available)\")\n",
    "            \n",
    "            if not articles:\n",
    "                print(f\"  âš ï¸  No articles found for {company} on {target_date}\")\n",
    "                continue\n",
    "            \n",
    "            # Map articles to your desired format and add to company collection\n",
    "            for article in articles:\n",
    "                source = article.get('source', {}).get('name', '')\n",
    "                author = article.get('author', '')\n",
    "                title = article.get('title', '')\n",
    "                description = article.get('description', '')\n",
    "                url = article.get('url', '')\n",
    "                published_at = article.get('publishedAt', '')\n",
    "\n",
    "                arc = {\n",
    "                    \"source\": source,\n",
    "                    \"author\": author,\n",
    "                    \"headline\": title,\n",
    "                    \"description\": description,\n",
    "                    \"url\": url,\n",
    "                    \"timestamp\": published_at,\n",
    "                    \"scraped_date\": target_date  # Add which date this was scraped from\n",
    "                }\n",
    "                company_articles[company].append(arc)\n",
    "            \n",
    "            # Small delay to be respectful to the API\n",
    "            import time\n",
    "            time.sleep(0.5)\n",
    "    \n",
    "    # Save one file per company with all articles from 7 days\n",
    "    total_files_created = 0\n",
    "    for company in QUERY:\n",
    "        if company_articles[company]:  # Only create file if there are articles\n",
    "            file_name = f\"{scrape_date}_{company}.json\"\n",
    "            path = os.path.join(r'../data/raw/news_api', file_name)\n",
    "\n",
    "            with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(company_articles[company], f, ensure_ascii=False, indent=4)\n",
    "            \n",
    "            print(f\"âœ… Saved {len(company_articles[company])} articles for {company} to: {file_name}\")\n",
    "            total_files_created += 1\n",
    "        else:\n",
    "            print(f\"âš ï¸  No articles found for {company} across all dates\")\n",
    "    \n",
    "    print(f\"\\nğŸ‰ Scraping completed!\")\n",
    "    print(f\"ğŸ“ Total files created: {total_files_created}\")\n",
    "    print(f\"ğŸ“ Files saved in: ../data/raw/news_api/\")\n",
    "    \n",
    "    # Summary\n",
    "    total_articles = sum(len(articles) for articles in company_articles.values())\n",
    "    print(f\"ğŸ“ˆ Total articles collected: {total_articles}\")\n",
    "    for company in QUERY:\n",
    "        print(f\"  â€¢ {company}: {len(company_articles[company])} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b8d9c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting News Scraping Process\n",
      "==================================================\n",
      "Date range: 2025-09-22 to 2025-09-16 (7 days)\n",
      "Companies: NVIDIA, AMD, Intel\n",
      "Max articles per company per day: 20\n",
      "==================================================\n",
      "Starting day-by-day news scraping...\n",
      "Target dates: ['2025-09-22', '2025-09-21', '2025-09-20', '2025-09-19', '2025-09-18', '2025-09-17', '2025-09-16']\n",
      "Companies: NVIDIA, AMD, Intel\n",
      "Max articles per company per day: 20\n",
      "--------------------------------------------------\n",
      "\n",
      "ğŸ“… Scraping news for 2025-09-22\n",
      "  ğŸ” Fetching NVIDIA news...\n",
      "Starting day-by-day news scraping...\n",
      "Target dates: ['2025-09-22', '2025-09-21', '2025-09-20', '2025-09-19', '2025-09-18', '2025-09-17', '2025-09-16']\n",
      "Companies: NVIDIA, AMD, Intel\n",
      "Max articles per company per day: 20\n",
      "--------------------------------------------------\n",
      "\n",
      "ğŸ“… Scraping news for 2025-09-22\n",
      "  ğŸ” Fetching NVIDIA news...\n",
      "  ğŸ“Š NVIDIA: 20 articles fetched (out of 508 available)\n",
      "  ğŸ“Š NVIDIA: 20 articles fetched (out of 508 available)\n",
      "  ğŸ” Fetching AMD news...\n",
      "  ğŸ” Fetching AMD news...\n",
      "  ğŸ“Š AMD: 20 articles fetched (out of 101 available)\n",
      "  ğŸ“Š AMD: 20 articles fetched (out of 101 available)\n",
      "  ğŸ” Fetching Intel news...\n",
      "  ğŸ” Fetching Intel news...\n",
      "  ğŸ“Š Intel: 20 articles fetched (out of 192 available)\n",
      "  ğŸ“Š Intel: 20 articles fetched (out of 192 available)\n",
      "\n",
      "ğŸ“… Scraping news for 2025-09-21\n",
      "  ğŸ” Fetching NVIDIA news...\n",
      "\n",
      "ğŸ“… Scraping news for 2025-09-21\n",
      "  ğŸ” Fetching NVIDIA news...\n",
      "  ğŸ“Š NVIDIA: 20 articles fetched (out of 109 available)\n",
      "  ğŸ“Š NVIDIA: 20 articles fetched (out of 109 available)\n",
      "  ğŸ” Fetching AMD news...\n",
      "  ğŸ” Fetching AMD news...\n",
      "  ğŸ“Š AMD: 20 articles fetched (out of 38 available)\n",
      "  ğŸ“Š AMD: 20 articles fetched (out of 38 available)\n",
      "  ğŸ” Fetching Intel news...\n",
      "  ğŸ” Fetching Intel news...\n",
      "  ğŸ“Š Intel: 20 articles fetched (out of 77 available)\n",
      "  ğŸ“Š Intel: 20 articles fetched (out of 77 available)\n",
      "\n",
      "ğŸ“… Scraping news for 2025-09-20\n",
      "  ğŸ” Fetching NVIDIA news...\n",
      "\n",
      "ğŸ“… Scraping news for 2025-09-20\n",
      "  ğŸ” Fetching NVIDIA news...\n",
      "  ğŸ“Š NVIDIA: 20 articles fetched (out of 128 available)\n",
      "  ğŸ“Š NVIDIA: 20 articles fetched (out of 128 available)\n",
      "  ğŸ” Fetching AMD news...\n",
      "  ğŸ” Fetching AMD news...\n",
      "  ğŸ“Š AMD: 20 articles fetched (out of 35 available)\n",
      "  ğŸ“Š AMD: 20 articles fetched (out of 35 available)\n",
      "  ğŸ” Fetching Intel news...\n",
      "  ğŸ” Fetching Intel news...\n",
      "  ğŸ“Š Intel: 20 articles fetched (out of 193 available)\n",
      "  ğŸ“Š Intel: 20 articles fetched (out of 193 available)\n",
      "\n",
      "ğŸ“… Scraping news for 2025-09-19\n",
      "  ğŸ” Fetching NVIDIA news...\n",
      "\n",
      "ğŸ“… Scraping news for 2025-09-19\n",
      "  ğŸ” Fetching NVIDIA news...\n",
      "  ğŸ“Š NVIDIA: 20 articles fetched (out of 759 available)\n",
      "  ğŸ“Š NVIDIA: 20 articles fetched (out of 759 available)\n",
      "  ğŸ” Fetching AMD news...\n",
      "  ğŸ” Fetching AMD news...\n",
      "  ğŸ“Š AMD: 20 articles fetched (out of 90 available)\n",
      "  ğŸ“Š AMD: 20 articles fetched (out of 90 available)\n",
      "  ğŸ” Fetching Intel news...\n",
      "  ğŸ” Fetching Intel news...\n",
      "  ğŸ“Š Intel: 20 articles fetched (out of 866 available)\n",
      "  ğŸ“Š Intel: 20 articles fetched (out of 866 available)\n",
      "\n",
      "ğŸ“… Scraping news for 2025-09-18\n",
      "  ğŸ” Fetching NVIDIA news...\n",
      "\n",
      "ğŸ“… Scraping news for 2025-09-18\n",
      "  ğŸ” Fetching NVIDIA news...\n",
      "  ğŸ“Š NVIDIA: 20 articles fetched (out of 1104 available)\n",
      "  ğŸ“Š NVIDIA: 20 articles fetched (out of 1104 available)\n",
      "  ğŸ” Fetching AMD news...\n",
      "  ğŸ” Fetching AMD news...\n",
      "  ğŸ“Š AMD: 20 articles fetched (out of 135 available)\n",
      "  ğŸ“Š AMD: 20 articles fetched (out of 135 available)\n",
      "  ğŸ” Fetching Intel news...\n",
      "  ğŸ” Fetching Intel news...\n",
      "  ğŸ“Š Intel: 20 articles fetched (out of 908 available)\n",
      "  ğŸ“Š Intel: 20 articles fetched (out of 908 available)\n",
      "\n",
      "ğŸ“… Scraping news for 2025-09-17\n",
      "  ğŸ” Fetching NVIDIA news...\n",
      "\n",
      "ğŸ“… Scraping news for 2025-09-17\n",
      "  ğŸ” Fetching NVIDIA news...\n",
      "  ğŸ“Š NVIDIA: 20 articles fetched (out of 774 available)\n",
      "  ğŸ“Š NVIDIA: 20 articles fetched (out of 774 available)\n",
      "  ğŸ” Fetching AMD news...\n",
      "  ğŸ” Fetching AMD news...\n",
      "  ğŸ“Š AMD: 20 articles fetched (out of 141 available)\n",
      "  ğŸ“Š AMD: 20 articles fetched (out of 141 available)\n",
      "  ğŸ” Fetching Intel news...\n",
      "  ğŸ” Fetching Intel news...\n",
      "  ğŸ“Š Intel: 20 articles fetched (out of 133 available)\n",
      "  ğŸ“Š Intel: 20 articles fetched (out of 133 available)\n",
      "\n",
      "ğŸ“… Scraping news for 2025-09-16\n",
      "  ğŸ” Fetching NVIDIA news...\n",
      "\n",
      "ğŸ“… Scraping news for 2025-09-16\n",
      "  ğŸ” Fetching NVIDIA news...\n",
      "  ğŸ“Š NVIDIA: 20 articles fetched (out of 454 available)\n",
      "  ğŸ“Š NVIDIA: 20 articles fetched (out of 454 available)\n",
      "  ğŸ” Fetching AMD news...\n",
      "  ğŸ” Fetching AMD news...\n",
      "  ğŸ“Š AMD: 20 articles fetched (out of 91 available)\n",
      "  ğŸ“Š AMD: 20 articles fetched (out of 91 available)\n",
      "  ğŸ” Fetching Intel news...\n",
      "  ğŸ” Fetching Intel news...\n",
      "  ğŸ“Š Intel: 20 articles fetched (out of 152 available)\n",
      "  ğŸ“Š Intel: 20 articles fetched (out of 152 available)\n",
      "âœ… Saved 140 articles for NVIDIA to: 20250923_155347_NVIDIA.json\n",
      "âœ… Saved 140 articles for AMD to: 20250923_155347_AMD.json\n",
      "âœ… Saved 140 articles for Intel to: 20250923_155347_Intel.json\n",
      "\n",
      "ğŸ‰ Scraping completed!\n",
      "ğŸ“ Total files created: 3\n",
      "ğŸ“ Files saved in: ../data/raw/news_api/\n",
      "ğŸ“ˆ Total articles collected: 420\n",
      "  â€¢ NVIDIA: 140 articles\n",
      "  â€¢ AMD: 140 articles\n",
      "  â€¢ Intel: 140 articles\n",
      "âœ… Saved 140 articles for NVIDIA to: 20250923_155347_NVIDIA.json\n",
      "âœ… Saved 140 articles for AMD to: 20250923_155347_AMD.json\n",
      "âœ… Saved 140 articles for Intel to: 20250923_155347_Intel.json\n",
      "\n",
      "ğŸ‰ Scraping completed!\n",
      "ğŸ“ Total files created: 3\n",
      "ğŸ“ Files saved in: ../data/raw/news_api/\n",
      "ğŸ“ˆ Total articles collected: 420\n",
      "  â€¢ NVIDIA: 140 articles\n",
      "  â€¢ AMD: 140 articles\n",
      "  â€¢ Intel: 140 articles\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸš€ Starting News Scraping Process\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Date range: {date_list[0]} to {date_list[-1]} (7 days)\")\n",
    "print(f\"Companies: {', '.join(QUERY)}\")\n",
    "print(f\"Max articles per company per day: 20\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "confirm = input(\"Proceed with scraping? (y/n): \")\n",
    "if confirm.lower() == 'y':\n",
    "    main()\n",
    "else:\n",
    "    print(\"Scraping cancelled.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
