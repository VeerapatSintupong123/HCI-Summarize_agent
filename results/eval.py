import os
from dotenv import load_dotenv
from smolagents import ToolCallingAgent, InferenceClientModel, tool, OpenAIServerModel
from agents.leader import HF_LEADER_MODEL_ID
from chunk_news.vector_db import get_retriever
from datetime import datetime
from tavily import TavilyClient
from pathlib import Path
import json

# --- Load env ---
load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "")
HF_WORKER_MODEL_ID = os.getenv("HF_WORKER_MODEL_ID", "gemini-2.5-flash")
HF_LEADER_MODEL_ID = os.getenv("HF_LEADER_MODEL_ID", "gemini-2.5-flash")

# ---------------------------
# Load Data
# ---------------------------
current_dir = Path(__file__).parent
result_path = current_dir.parent / "results" / "ex4_result.json"

if not result_path.exists():
    raise FileNotFoundError(f"‚ùå File not found: {result_path}")

with open(result_path, "r", encoding="utf-8") as f:
    news_data = json.load(f)

# --- Model ---
model = OpenAIServerModel(
    model_id=HF_LEADER_MODEL_ID,
    api_base="https://generativelanguage.googleapis.com/v1beta/",
    api_key=GEMINI_API_KEY,
)


# --- (Your existing code for imports, loading env, loading data, and defining the model) ---

evaluation_worker_agent = ToolCallingAgent(
    model=model,
    tools=[],
    name="EvaluationAgent",
    description="evaluates the quality of summaries and impact/trend analyses generated by other agents based on provided articles. It assesses summaries for relevance, coherence, conciseness, and faithfulness, and evaluates impact/trend analyses for correctness, depth, logical consistency, and actionable insights. The agent provides detailed feedback and scores for each criterion in a structured JSON format.",
    stream_outputs=False,
)

# This is your static prompt template
prompt_template = """
CRITICAL: Your entire response MUST be a single, valid JSON object and nothing else. Your response must begin with '{' and end with '}'.

You are an expert evaluator, acting as an LLM-as-a-Judge. Your task is to meticulously analyze a provided JSON object containing an article, its generated summary, and an impact/trend analysis. You will evaluate the `worker_summary` and `impact_trend` fields based on the specific criteria below and provide your findings in a structured JSON format.

**Input to Judge:**
The JSON object to be evaluated is provided at the end of this prompt. It contains:
- `content`: The full original article text. This is your primary source of truth.
- `summary`: The generated summary to be evaluated.
- `financial_impact_trend`: The generated impact/trend analysis to be evaluated.

**Instructions:**

**Step 1: Evaluate the Summary**
Focus on the `summary` field. Compare it against the `content` of the article and evaluate it based on the following four criteria. For each criterion, provide a score from 0.0 (very poor) to 1.0 (excellent) and a brief justification.

* **Relevance (‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á):** Does the summary capture the main, critical points of the source text? Does it omit key information or include unimportant details?
* **Coherence (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á):** Is the summary easy to read, logical, and well-structured? Is the flow of sentences smooth?
* **Conciseness (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö):** Is the summary of an appropriate length relative to the source text? Is it free of redundant words or phrases?
* **Faithfulness / Factuality (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á):** Does the summary accurately represent the information from the source text without distortion? Are there any "hallucinations" or fabricated details? (Score 0.0 for any clear factual error).

**Step 2: Evaluate the Impact/Trend Analysis**
Focus on the `financial_impact_trend` field. Analyze its quality and validity against the `content` of the article based on the following four criteria. For each criterion, provide a score from 0.0 (very poor) to 1.0 (excellent) and a brief justification.

* **Correctness (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á):** Is the analysis based on correct information from the source text? Are there misinterpretations or distortions?
* **Depth (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∂‡∏Å‡∏ã‡∏∂‡πâ‡∏á):** Does the analysis go beyond surface-level statements? Does it explore root causes, consequences, or significant hidden implications?
* **Logical Consistency (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡πÄ‡∏´‡∏ï‡∏∏‡∏™‡∏°‡∏ú‡∏•):** Is the reasoning sound and internally consistent? Are there any self-contradictions?
* **Actionable Insights (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å‡∏ó‡∏µ‡πà‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ):** Does the analysis lead to clear conclusions, identify opportunities, or specify problems that could inform a decision or action?

**Step 3: Final JSON Output Format**
After completing your evaluation, structure your entire response into the following nested JSON format. Provide a score and a brief reasoning for each metric as determined in the steps above.

{
  "summary_evaluation": {
    "relevance": {
      "score": <float_from_0.0_to_1.0>,
      "reasoning": "<brief justification for relevance>"
    },
    "coherence": {
      "score": <float_from_0.0_to_1.0>,
      "reasoning": "<brief justification for coherence>"
    },
    "conciseness": {
      "score": <float_from_0.0_to_1.0>,
      "reasoning": "<brief justification for conciseness>"
    },
    "faithfulness": {
      "score": <float_from_0.0_to_1.0>,
      "reasoning": "<brief justification for faithfulness>"
    }
  },
  "impact_trend_evaluation": {
    "correctness": {
      "score": <float_from_0.0_to_1.0>,
      "reasoning": "<brief justification for correctness>"
    },
    "depth": {
      "score": <float_from_0.0_to_1.0>,
      "reasoning": "<brief justification for depth>"
    },
    "logical_consistency": {
      "score": <float_from_0.0_to_1.0>,
      "reasoning": "<brief justification for logical consistency>"
    },
    "actionable_insights": {
      "score": <float_from_0.0_to_1.0>,
      "reasoning": "<brief justification for actionable insights>"
    }
  }
}
"""

# --- NEW CODE BLOCK TO LOOP, EVALUATE, AND SAVE ---

all_evaluations = []
# 1. Loop through each news item loaded from your JSON file
for news_item in news_data:
    print(f"üì∞ Evaluating headline: '{news_item['headline']}'...")

    # 2. Convert the current news item (Python dict) into a JSON string
    input_json_string = json.dumps(news_item, indent=2)

    # 3. Create the final prompt by combining the template and the specific JSON data for this item
    final_prompt = f"{prompt_template}\n\n**JSON object to evaluate:**\n{input_json_string}"

    # 4. Call the agent with the complete prompt (template + data)
    try:
        evaluation_result_str = evaluation_worker_agent.run(final_prompt)
        # The agent returns a JSON string, so we parse it into a Python dictionary
        evaluation_result_json = json.loads(evaluation_result_str)

        # 5. Store the result along with the original headline for context
        all_evaluations.append({
            "headline": news_item["headline"],
            "evaluation": evaluation_result_json
        })
        print("‚úÖ Evaluation successful.")

    except Exception as e:
        print(f"‚ùå Failed to evaluate item. Error: {e}")
        all_evaluations.append({
            "headline": news_item["headline"],
            "evaluation": f"ERROR: {str(e)}"
        })

# 6. Save all the collected evaluations into a new JSON file
output_path = current_dir.parent / "results" / "ex4_evaluations.json"
with open(output_path, "w", encoding="utf-8") as f:
    json.dump(all_evaluations, f, indent=2, ensure_ascii=False)

print(f"\n‚ú® All evaluations complete. Results saved to: {output_path}")